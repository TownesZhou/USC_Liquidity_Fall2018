{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that checks tensor size\n",
    "\n",
    "def assertSize(tensor, expected_size, tensor_type_msg = None):\n",
    "    if tensor_type_msg == None:\n",
    "        msg = \"TensorSizeMismatch: Expect tensor size {}, but received {}.\" \\\n",
    "            .format(expected_size, tensor.size())\n",
    "    else:\n",
    "        msg = \"TensorSizeMismatch: Expect {} size {}, but received {}.\" \\\n",
    "            .format(tensor_type_msg, expected_size, tensor.size())\n",
    "    assert(tensor.size() == expected_size), msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA-RNN Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Attention Mechanism Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Attention Mechanism Module \n",
    "\n",
    "class InputAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_driving, window_size, en_hidden_size):\n",
    "        super(InputAttention, self).__init__()\n",
    "        self.n = num_driving\n",
    "        self.T = window_size\n",
    "        self.m = en_hidden_size\n",
    "        \n",
    "        self.FC1 = nn.Linear(2 * self.m + self.T, self.T)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.FC2 = nn.Linear(self.T, 1)\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X, en_hidden, en_cell, time_step):\n",
    "        batch_size = X.size()[0]\n",
    "        # input driving series matrix X is of dimension: (batch_size, n, T)\n",
    "        # encoder hiden state and cell state vector is of dimension: (batch_size, m)\n",
    "        assertSize(X, (batch_size, self.n, self.T), \"InAttn: Input driving series matrix\") \n",
    "        assertSize(en_hidden, (batch_size, self.m), \"InAttn: Encoder hidden state vector\")\n",
    "        assertSize(en_cell, (batch_size, self.m), \"InAttn: Encoder cell state vector\")\n",
    "        \n",
    "        # Concatenate the driving series to the end of the encoder LSTM's\n",
    "        #   hidden state and cell state vectors\n",
    "        en_hidden = en_hidden.unsqueeze(1).expand((batch_size, self.n, self.m))\n",
    "        en_cell = en_cell.unsqueeze(1).expand((batch_size, self.n, self.m))\n",
    "        X_concat = torch.cat([en_hidden, en_cell, X], dim = 2)\n",
    "        # Now, X is of dimension: (batch_size, n, 2m+T)\n",
    "        assertSize(X_concat, (batch_size, self.n, 2 * self.m + self.T), \"InAttn: Concatenated driving series matrix\")\n",
    "        \n",
    "        # Apply the first affine transformation on the input n driving series\n",
    "        z = X_concat\n",
    "        z = self.FC1(z)\n",
    "        assertSize(z, (batch_size, self.n, self.T), \"InAttn: First affine vector\")\n",
    "        # Apply tanh activation function\n",
    "        z = self.Tanh(z)\n",
    "        # Apply the second affine transformation\n",
    "        z = self.FC2(z)\n",
    "        assertSize(z, (batch_size, self.n, 1), \"InAttn: Second affine vector\")\n",
    "        # Apply softmax activation function and squeeze the result tensor\n",
    "        z = self.Softmax(z)\n",
    "        z = z.squeeze()\n",
    "        assertSize(z, (batch_size, self.n), \"InAttn: Result weight vector\")\n",
    "        \n",
    "        # Compute the weighted extracted driving series at time time_step\n",
    "        X_t = X[:, :, time_step]\n",
    "        assertSize(X_t, (batch_size, self.n), \"InAttn: Driving series at time_step \")\n",
    "        X_tild = z * X_t    # tensor element-wise multiplication\n",
    "        \n",
    "        # Return weighted extracted driving series at time time_step\n",
    "        # X_tild is of dimension: (batch_size, self.n)\n",
    "        assertSize(X_tild, (batch_size, self.n))\n",
    "        \n",
    "        return X_tild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder LSTM Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder LSTM Module\n",
    "# Using InputAttention as submodule\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_driving, window_size, en_hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.n = num_driving\n",
    "        self.T = window_size\n",
    "        self.m = en_hidden_size\n",
    "        \n",
    "        self.attn = InputAttention(self.n, self.T, self.m)\n",
    "        self.LSTMCell = nn.LSTMCell(self.n, self.m)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        batch_size = X.size()[0]\n",
    "        # input driving series matrix X is of dimension: (batch_size, n, T)\n",
    "        assertSize(X, (batch_size, self.n, self.T), \"Encoder: Input driving series matrix\")\n",
    "        \n",
    "        # Initialize hidden state and cell state vectors\n",
    "        # hiden state and cell state vectors are of dimension: (batch_size, m)\n",
    "        h, c = [torch.zeros(batch_size, self.m)] * 2\n",
    "        # Store all historical hidden state vectors\n",
    "        hidden_states = h.unsqueeze(1)\n",
    "        assertSize(hidden_states, (batch_size, 1, self.m), \"Encoder: Initial hidden states recorder\")\n",
    "        \n",
    "        # Forward propagation through timesteps\n",
    "        for t in range(self.T):\n",
    "            # Compute the weighted extracted driving series\n",
    "            X_tild = self.attn(X, h, c, t)\n",
    "            # Compute the next hidden and cell states through LSTM cell\n",
    "            h, c = self.LSTMCell(X_tild, (h, c))\n",
    "            # Store the current hidden state\n",
    "            hidden_states = torch.cat([hidden_states, h.unsqueeze(1)], dim = 1)\n",
    "            \n",
    "        # Discard the first (initial) hidden and cell states in the recorder\n",
    "        # The recorder matrix is of dimension: (batch_size, T, m)\n",
    "        hidden_states = hidden_states[:, 1:, :]\n",
    "        assertSize(hidden_states, (batch_size, self.T, self.m), \"Encoder: hidden states matrix\")\n",
    "        \n",
    "        # Return the hidden state matrix of length T\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Attention Mechanism Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Attention Mechanism Module\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size, en_hidden_size, de_hidden_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.T = window_size\n",
    "        self.m = en_hidden_size\n",
    "        self.p = de_hidden_size\n",
    "        \n",
    "        self.FC1 = nn.Linear(2 * self.p + self.m, self.m)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.FC2 = nn.Linear(self.m, 1)\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, hidden_states, de_hidden, de_cell, time_step):\n",
    "        batch_size = hidden_states.size()[0]\n",
    "        # input encoder hidden states tensor is of dimension: (batch_size, T, m)\n",
    "        # decoder hidden state and cell state vectors are of dimension: (batch_size, p)\n",
    "        assertSize(hidden_states, (batch_size, self.T, self.m), \"TempAttn: Input encoder hidden states\")\n",
    "        assertSize(de_hidden, (batch_size, self.p), \"TempAttn: Decoder hidden states vector\")\n",
    "        assertSize(de_cell, (batch_size, self.p), \"TempAttn: Decoder cell states vector\")\n",
    "        \n",
    "        # Concatenate the input encoder hidden states to the end of the decoder LSTM's\n",
    "        #   hidden state and cell state vectors\n",
    "        de_hidden = de_hidden.unsqueeze(1).expand((batch_size, self.T, self.p))\n",
    "        de_cell = de_cell.unsqueeze(1).expand((batch_size, self.T, self.p))\n",
    "        h_concat = torch.cat([de_hidden, de_cell, hidden_states], dim = 2)\n",
    "        # Now, h_concat is of dimension: (batch_size, T, 2p+m)\n",
    "        assertSize(h_concat, (batch_size, self.T, 2 * self.p + self.m), \"TempAttn: Concatenated hidden states matrix\")\n",
    "        \n",
    "        # Apply the first affine transformation on the input n driving series\n",
    "        z = h_concat\n",
    "        z = self.FC1(z)\n",
    "        assertSize(z, (batch_size, self.T, self.m), \"TempAttn: First affine vector\")\n",
    "        # Apply tanh activation function\n",
    "        z = self.Tanh(z)\n",
    "        # Apply the second affine transformation\n",
    "        z = self.FC2(z)\n",
    "        assertSize(z, (batch_size, self.T, 1), \"TempAttn: Second affine vector\")\n",
    "        # Apply softmax activation function and squeeze the result tensor\n",
    "        z = self.Softmax(z)\n",
    "        \n",
    "        # Compute the weighted summation of encoder hidden states as the context vector\n",
    "        C = hidden_states * z      # element-wise multiplication, broadcast on the last dimension\n",
    "        assertSize(C, (batch_size, self.T, self.m), \"TempAttn: weighted input encoder hidden states matrix\")\n",
    "        C = torch.sum(C, 1) # Summation across T timesteps\n",
    "        \n",
    "        # Return the context vector\n",
    "        # C is of dimension: (batch_size, m)\n",
    "        assertSize(C, (batch_size, self.m), \"TempAttn: context vector\")\n",
    "        \n",
    "        return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder LSTM Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder LSTM Module\n",
    "# using TemporalAttention as submodule\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size, en_hidden_size, de_hidden_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.T = window_size\n",
    "        self.m = en_hidden_size\n",
    "        self.p = de_hidden_size\n",
    "        \n",
    "        self.attn = TemporalAttention(self.T, self.m, self.p)\n",
    "        self.FCin = nn.Linear(self.m + 1, 1)\n",
    "        self.LSTMCell = nn.LSTMCell(1, self.p)\n",
    "        self.FCout1 = nn.Linear(self.p + self.m, self.p)\n",
    "        self.ReLU = nn.ELU()\n",
    "        self.FCout2 = nn.Linear(self.p, 1)\n",
    "        \n",
    "    def forward(self, hidden_states, y, target_length, teacher_forcing = True):\n",
    "        batch_size = hidden_states.size()[0]\n",
    "        # Input encoder hidden states matrix is of dimension: (batch_size, T, m)\n",
    "        assertSize(hidden_states, (batch_size, self.T, self.m), \"Decoder: Input encoder hiden states matrix\")\n",
    "        # target series y is of dimension: (batch_size, target_length)\n",
    "        # Note:\n",
    "        ##      - All target series within the current minibatch should have fixed target_length = S\n",
    "        ##      - The first element of the target series is used to produce the first predicton\n",
    "        ##        Prediction series and target series both have length S, but shift in one time_step\n",
    "        ##            i.e. Prediction[t] <==> target[t + 1]\n",
    "        assertSize(y, (batch_size, target_length), \"Decoder: Input target series vectors\")\n",
    "        \n",
    "        # Initialize hidden state and cell state vectors\n",
    "        # hidden state d and cell state s are of dimension: (batch_size, p)\n",
    "        d, s = [torch.zeros(batch_size, self.p)] * 2\n",
    "        \n",
    "        # Initialize prediction value to the (dummy) first element of the target series\n",
    "        y_pred = y[:, 0]\n",
    "        # Initialize history container to store all prediction values across target_length\n",
    "        # history container is of dimension: (batch_size, target_length)\n",
    "        y_history = y_pred.unsqueeze(1)\n",
    "        assertSize(y_history, (batch_size, 1), \"Decoder: Initial y_history\")\n",
    "        \n",
    "        # Forward propagation through timesteps\n",
    "        for t in range(target_length):\n",
    "            # Obtain the context vector at current step\n",
    "            C = self.attn(hidden_states, d, s, t)\n",
    "            \n",
    "            # Concatenate the context vector to the end of input target value\n",
    "            if teacher_forcing:\n",
    "                # If using teacher forcing, extract the input target value from the target series\n",
    "                y_value = y[:, t]\n",
    "            else:\n",
    "                # If not using teacher forcing, use the last prediction value as current\n",
    "                #   target value\n",
    "                y_value = y_pred\n",
    "            # y_value is of dimension: (batch_size, )\n",
    "            assertSize(y_value, (batch_size, ), \"Decoder: Current input target value\")\n",
    "            y_value = y_value.unsqueeze(1)\n",
    "            # Concatenate. After concatenation, y_concat is of dimension: (batch_size, m + 1)\n",
    "            y_concat = torch.cat([y_value, C], dim = 1)\n",
    "            assertSize(y_concat, (batch_size, self.m + 1), \"Decoder: Concatenated input series\")\n",
    "            \n",
    "            # Apply the first affine transformation on input series to get the input for LSTM\n",
    "            LSTM_in = self.FCin(y_concat)\n",
    "            \n",
    "            # Compute the next hidden and cell state through the LSTM unit\n",
    "            d, s = self.LSTMCell(LSTM_in, (d, s))\n",
    "            \n",
    "            # Concatenate the hidden state vector to the end of context vector\n",
    "            # The concatenated vector is of dimension: (batch_size, p + m)\n",
    "            FC_in = torch.cat([d, C], dim = 1)\n",
    "            assertSize(FC_in, (batch_size, self.p + self.m), \"Decoder: Concatenated hidden and context vector\")\n",
    "            \n",
    "            # Apply the second affine transformation on the concatenated output of LSTM unit\n",
    "            # Output is of dimension: (batch_size, p)\n",
    "            z = self.FCout1(FC_in)\n",
    "            assertSize(z, (batch_size, self.p), \"Decoder: Output of first affine transform after LSTM\")\n",
    "            # Apply the ReLU activation function (Note: This step was not in the paper!)\n",
    "            z = self.ReLU(z)\n",
    "            # Apply the third affine transformation\n",
    "            # output is of dimension: (batch_size, 1)\n",
    "            z = self.FCout2(z)\n",
    "            assertSize(z, (batch_size, 1), \"Decoder: Output of second affine transform after LSTM\")\n",
    "            \n",
    "            # Final Prediction\n",
    "            y_pred = z.squeeze()\n",
    "            # Add prediction value to history\n",
    "            y_history = torch.cat([y_history, z], dim = 1)\n",
    "            \n",
    "        # Discard the first element in the history of prediction values.\n",
    "        y_history = y_history[:, 1:]\n",
    "        # history is of dimension: (batch_size, target_length)\n",
    "        assertSize(y_history, (batch_size, target_length), \"Decoder: Final prediction value series\")\n",
    "        \n",
    "        return y_history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA-RNN wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the entire DA-RNN, a wrapper class combining encoder and decoder\n",
    "\n",
    "class DARNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_driving, window_size, en_hidden, de_hidden):\n",
    "        super(DARNN, self).__init__()\n",
    "        self.n = num_driving\n",
    "        self.T = window_size\n",
    "        self.m = en_hidden\n",
    "        self.p = de_hidden\n",
    "        \n",
    "        self.encoder = EncoderLSTM(self.n, self.T, self.m)\n",
    "        self.decoder = DecoderLSTM(self.T, self.m, self.p)\n",
    "        \n",
    "    def forward(self, X, y, target_length, teacher_forcing = True):\n",
    "        # Forward through encoder\n",
    "        hidden_states = self.encoder(X)\n",
    "        # Forward through decoder\n",
    "        y_history = self.decoder(hidden_states, y, target_length, teacher_forcing)\n",
    "        \n",
    "        # Return the final prediction\n",
    "        return y_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random input and output\n",
    "\n",
    "batch_size = 16\n",
    "n = 10\n",
    "T = 5\n",
    "S = 7\n",
    "m = 3\n",
    "p = 6\n",
    "\n",
    "X = torch.randn(batch_size, n, T)\n",
    "y = torch.randn(batch_size, S)\n",
    "\n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7])\n"
     ]
    }
   ],
   "source": [
    "model = DARNN(n, T, m, p)\n",
    "\n",
    "y_history = model(X, y, S, teacher_forcing = True)\n",
    "\n",
    "print(y_history.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(size_average=False, reduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 5)\n",
    "y = torch.zeros(3, 5)\n",
    "\n",
    "loss = criterion(x, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, target_length, criterion, optimizer, teacher_forcing = True):\n",
    "    prediction = model(X, y, target_length, teacher_forcing = teacher_forcing)\n",
    "    loss = criterion(prediction, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
